{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharpQuagga/Colab_Notebooks/blob/master/Keras_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq1D9FcfkBOT",
        "colab_type": "code",
        "outputId": "8d2edbc5-1487-40cf-b94c-fd349ed5bedc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist\n",
        "import seaborn as sns\n",
        "from keras.initializers import RandomNormal"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufh41FCTlFpt",
        "colab_type": "code",
        "outputId": "25092a1c-65c2-4d1c-bde7-9d748293797d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su4J-7lWlWWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzPoeZY3lxbn",
        "colab_type": "code",
        "outputId": "48418e82-963b-4ad3-8a41-391488792345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n",
            " 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n",
            " 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n",
            "   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n",
            "  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n",
            " 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n",
            " 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            " 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n",
            " 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n",
            "  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgtQ0ognlzkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train/255\n",
        "X_test = X_test/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ1VTxTMl9L_",
        "colab_type": "code",
        "outputId": "4394c0d0-7b52-4d50-be2c-e651b0dbf871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
            " 0.96862745 0.49803922 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
            " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.19215686\n",
            " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
            " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
            " 0.96862745 0.94509804 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
            " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.04313725\n",
            " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.1372549  0.94509804\n",
            " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
            " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
            " 0.58823529 0.10588235 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
            " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15294118 0.58039216\n",
            " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.67058824\n",
            " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
            " 0.31372549 0.03529412 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.53333333 0.99215686\n",
            " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUjkNI52mBuP",
        "colab_type": "code",
        "outputId": "c14e448b-ba23-4508-f166-17e424a8ac87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Class label of first image: \", y_train[0])\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(\"After conversion :\", y_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class label of first image:  5\n",
            "After conversion : [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQd8Wf02mjPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0WKR1eqnJKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_dim = 10\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "batch_size = 128\n",
        "np_epoch = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuTkylR4q4L_",
        "colab_type": "text"
      },
      "source": [
        "## **Sequential Model (only softmax layer)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKcsUbZenQeM",
        "colab_type": "code",
        "outputId": "e7e8675d-653a-4d30-fc1d-7e37be79a37e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(output_dim,input_dim=input_dim, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzVRLFhlndC-",
        "colab_type": "code",
        "outputId": "1c7671fc-ae73-4a63-9cb4-8de0794e31b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=np_epoch, verbose=1, validation_data=(X_test, y_test\n",
        "                                                                                                          ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 1.2864 - acc: 0.6968 - val_loss: 0.8185 - val_acc: 0.8311\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.7177 - acc: 0.8408 - val_loss: 0.6114 - val_acc: 0.8598\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.5883 - acc: 0.8586 - val_loss: 0.5290 - val_acc: 0.8716\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.5266 - acc: 0.8677 - val_loss: 0.4826 - val_acc: 0.8788\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.4890 - acc: 0.8739 - val_loss: 0.4527 - val_acc: 0.8847\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 21us/step - loss: 0.4631 - acc: 0.8792 - val_loss: 0.4312 - val_acc: 0.8865\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.4440 - acc: 0.8825 - val_loss: 0.4147 - val_acc: 0.8903\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.4290 - acc: 0.8853 - val_loss: 0.4020 - val_acc: 0.8932\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.4169 - acc: 0.8882 - val_loss: 0.3911 - val_acc: 0.8961\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.4069 - acc: 0.8903 - val_loss: 0.3825 - val_acc: 0.8980\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.3983 - acc: 0.8923 - val_loss: 0.3747 - val_acc: 0.9007\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.3910 - acc: 0.8937 - val_loss: 0.3683 - val_acc: 0.9024\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.3845 - acc: 0.8954 - val_loss: 0.3628 - val_acc: 0.9027\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 22us/step - loss: 0.3789 - acc: 0.8967 - val_loss: 0.3577 - val_acc: 0.9048\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.3738 - acc: 0.8978 - val_loss: 0.3531 - val_acc: 0.9049\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.3692 - acc: 0.8990 - val_loss: 0.3491 - val_acc: 0.9059\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.3651 - acc: 0.8999 - val_loss: 0.3455 - val_acc: 0.9064\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 21us/step - loss: 0.3613 - acc: 0.9006 - val_loss: 0.3421 - val_acc: 0.9072\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 21us/step - loss: 0.3578 - acc: 0.9011 - val_loss: 0.3392 - val_acc: 0.9076\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 21us/step - loss: 0.3546 - acc: 0.9022 - val_loss: 0.3364 - val_acc: 0.9082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAx7uokztNw0",
        "colab_type": "text"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nV4j2QIoxR2",
        "colab_type": "code",
        "outputId": "62fc1767-7eb3-4e74-ba4a-2487e0b479b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test score :\", score[0])\n",
        "print(\"Train Score :\", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score : 0.33635264184474944\n",
            "Train Score : 0.9082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqKzfPCRpNCq",
        "colab_type": "text"
      },
      "source": [
        "## **MLP + Sigmoid + SGD Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGN16_VNpHtJ",
        "colab_type": "code",
        "outputId": "8e20331c-6f68-40bd-b4aa-5315b1d35185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model_sig = Sequential()\n",
        "model_sig.add(Dense(512, activation='sigmoid', input_shape=(input_dim, )))\n",
        "model_sig.add(Dense(128, activation='sigmoid'))\n",
        "model_sig.add(Dense(output_dim, activation='softmax'))\n",
        "\n",
        "model_sig.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 468,874\n",
            "Trainable params: 468,874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYOM9taBpocB",
        "colab_type": "code",
        "outputId": "c2614090-a956-4632-91bc-dae41ec789bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "model_sig.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history = model_sig.fit(X_train, y_train, batch_size=batch_size, epochs=np_epoch, verbose=1, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 2.2674 - acc: 0.2354 - val_loss: 2.2182 - val_acc: 0.3101\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 2.1730 - acc: 0.4781 - val_loss: 2.1154 - val_acc: 0.5129\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 2.0523 - acc: 0.5825 - val_loss: 1.9689 - val_acc: 0.6679\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 5s 90us/step - loss: 1.8791 - acc: 0.6459 - val_loss: 1.7627 - val_acc: 0.6695\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 1.6541 - acc: 0.6833 - val_loss: 1.5189 - val_acc: 0.7334\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 5s 90us/step - loss: 1.4165 - acc: 0.7223 - val_loss: 1.2922 - val_acc: 0.7455\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 5s 91us/step - loss: 1.2114 - acc: 0.7528 - val_loss: 1.1100 - val_acc: 0.7883\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 1.0523 - acc: 0.7775 - val_loss: 0.9728 - val_acc: 0.7903\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.9322 - acc: 0.7940 - val_loss: 0.8676 - val_acc: 0.8142\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.8401 - acc: 0.8110 - val_loss: 0.7873 - val_acc: 0.8203\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.7676 - acc: 0.8220 - val_loss: 0.7217 - val_acc: 0.8317\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.7092 - acc: 0.8318 - val_loss: 0.6699 - val_acc: 0.8469\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.6612 - acc: 0.8405 - val_loss: 0.6262 - val_acc: 0.8487\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.6216 - acc: 0.8476 - val_loss: 0.5898 - val_acc: 0.8562\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.5882 - acc: 0.8535 - val_loss: 0.5583 - val_acc: 0.8614\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.5599 - acc: 0.8586 - val_loss: 0.5318 - val_acc: 0.8654\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.5359 - acc: 0.8634 - val_loss: 0.5094 - val_acc: 0.8681\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.5150 - acc: 0.8676 - val_loss: 0.4907 - val_acc: 0.8729\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.4969 - acc: 0.8703 - val_loss: 0.4736 - val_acc: 0.8761\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.4811 - acc: 0.8735 - val_loss: 0.4581 - val_acc: 0.8796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAMkKEreqBg-",
        "colab_type": "code",
        "outputId": "e8a78e1b-2a28-4b4a-a84e-7c7f3560382d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "score = model_sig.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test score :\", score[0])\n",
        "print(\"Train Score :\", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score : 0.45814904708862303\n",
            "Train Score : 0.8796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KPpIlxNrI9X",
        "colab_type": "text"
      },
      "source": [
        "## **ReLU Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2NKQ9r_qsFt",
        "colab_type": "code",
        "outputId": "470c3dac-9fb8-44b2-ac8d-d3a4f25924bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "model_relu = Sequential()\n",
        "model_relu.add(Dense(512, activation='relu', input_shape=(input_dim, ),kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)))\n",
        "model_relu.add(Dense(128, activation='relu', input_shape=(input_dim, ),kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)))\n",
        "model_relu.add(Dense(output_dim, activation='softmax'))\n",
        "\n",
        "model_relu.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 468,874\n",
            "Trainable params: 468,874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2DQgR3dti7s",
        "colab_type": "text"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyfKNCg8LPve",
        "colab_type": "code",
        "outputId": "bca8ed2f-b48b-445e-c7a1-e5dfcbdf60a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "model_relu.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history = model_relu.fit(X_train, y_train, batch_size=batch_size, epochs=np_epoch, verbose=1, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 6s 107us/step - loss: 0.7411 - acc: 0.7715 - val_loss: 0.3979 - val_acc: 0.8860\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 6s 105us/step - loss: 0.3667 - acc: 0.8921 - val_loss: 0.3198 - val_acc: 0.9085\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.3061 - acc: 0.9106 - val_loss: 0.2788 - val_acc: 0.9187\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.2720 - acc: 0.9207 - val_loss: 0.2570 - val_acc: 0.9274\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.2483 - acc: 0.9270 - val_loss: 0.2386 - val_acc: 0.9326\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.2302 - acc: 0.9328 - val_loss: 0.2253 - val_acc: 0.9360\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 5s 91us/step - loss: 0.2151 - acc: 0.9374 - val_loss: 0.2128 - val_acc: 0.9388\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2023 - acc: 0.9413 - val_loss: 0.2031 - val_acc: 0.9404\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.1912 - acc: 0.9449 - val_loss: 0.1955 - val_acc: 0.9435\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.1818 - acc: 0.9468 - val_loss: 0.1883 - val_acc: 0.9458\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1731 - acc: 0.9498 - val_loss: 0.1802 - val_acc: 0.9478\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1654 - acc: 0.9521 - val_loss: 0.1750 - val_acc: 0.9491\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.1583 - acc: 0.9538 - val_loss: 0.1701 - val_acc: 0.9509\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.1520 - acc: 0.9560 - val_loss: 0.1651 - val_acc: 0.9512\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.1462 - acc: 0.9581 - val_loss: 0.1608 - val_acc: 0.9540\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.1407 - acc: 0.9596 - val_loss: 0.1582 - val_acc: 0.9538\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.1358 - acc: 0.9606 - val_loss: 0.1540 - val_acc: 0.9548\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.1310 - acc: 0.9618 - val_loss: 0.1518 - val_acc: 0.9557\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.1268 - acc: 0.9632 - val_loss: 0.1471 - val_acc: 0.9572\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1225 - acc: 0.9647 - val_loss: 0.1428 - val_acc: 0.9579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QR1gpButV9-",
        "colab_type": "code",
        "outputId": "fbc769e4-6749-444d-bde5-78c8a77bed24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "score = model_relu.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test score :\", score[0])\n",
        "print(\"Train Score :\", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score : 0.14277900661397724\n",
            "Train Score : 0.9579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHkA5tdKtCj2",
        "colab_type": "text"
      },
      "source": [
        "## **Relu + BatchNormalization + Dropout**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taZJN1OZsQrh",
        "colab_type": "code",
        "outputId": "1342a35d-9a8f-4bc2-d318-a194fd21ca77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.layers import Dropout\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "model_drop = Sequential()\n",
        "model_drop.add(Dense(512, activation='relu', input_shape=(input_dim, ),kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)))\n",
        "model_drop.add(BatchNormalization())\n",
        "model_drop.add(Dropout(0.5))\n",
        "model_drop.add(Dense(128, activation='relu', input_shape=(input_dim, ),kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)))\n",
        "model_drop.add(BatchNormalization())\n",
        "model_drop.add(Dropout(0.5))\n",
        "model_drop.add(Dense(output_dim, activation='softmax'))\n",
        "\n",
        "model_drop.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 471,434\n",
            "Trainable params: 470,154\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4j5aAbHjrUS",
        "colab_type": "code",
        "outputId": "a019fc06-6f96-43e8-c08a-f1d0a621e97e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "model_drop.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history = model_drop.fit(X_train, y_train, batch_size=batch_size, epochs=np_epoch, verbose=1, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 1.6591 - acc: 0.4870 - val_loss: 0.6081 - val_acc: 0.8317\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.8988 - acc: 0.7069 - val_loss: 0.4622 - val_acc: 0.8719\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.7394 - acc: 0.7624 - val_loss: 0.3998 - val_acc: 0.8878\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.6575 - acc: 0.7897 - val_loss: 0.3638 - val_acc: 0.8994\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.5990 - acc: 0.8101 - val_loss: 0.3365 - val_acc: 0.9069\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.5620 - acc: 0.8230 - val_loss: 0.3158 - val_acc: 0.9117\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.5287 - acc: 0.8343 - val_loss: 0.2985 - val_acc: 0.9164\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.5028 - acc: 0.8420 - val_loss: 0.2854 - val_acc: 0.9194\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.4818 - acc: 0.8505 - val_loss: 0.2745 - val_acc: 0.9219\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.4646 - acc: 0.8555 - val_loss: 0.2647 - val_acc: 0.9250\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 7s 110us/step - loss: 0.4553 - acc: 0.8605 - val_loss: 0.2566 - val_acc: 0.9269\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 7s 124us/step - loss: 0.4377 - acc: 0.8636 - val_loss: 0.2490 - val_acc: 0.9290\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.4277 - acc: 0.8669 - val_loss: 0.2414 - val_acc: 0.9309\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.4144 - acc: 0.8718 - val_loss: 0.2351 - val_acc: 0.9316\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.4050 - acc: 0.8739 - val_loss: 0.2299 - val_acc: 0.9339\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.3963 - acc: 0.8769 - val_loss: 0.2244 - val_acc: 0.9346\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.3901 - acc: 0.8779 - val_loss: 0.2203 - val_acc: 0.9362\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 7s 118us/step - loss: 0.3768 - acc: 0.8841 - val_loss: 0.2159 - val_acc: 0.9365\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 7s 117us/step - loss: 0.3707 - acc: 0.8854 - val_loss: 0.2123 - val_acc: 0.9381\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 7s 116us/step - loss: 0.3619 - acc: 0.8887 - val_loss: 0.2078 - val_acc: 0.9392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c8wSa-vtfmP",
        "colab_type": "text"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr4ZaPoTsxkR",
        "colab_type": "code",
        "outputId": "8d6fab76-3cea-4980-b70b-109324f3caa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "score = model_drop.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test score :\", score[0])\n",
        "print(\"Train Score :\", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score : 0.20783675619959832\n",
            "Train Score : 0.9392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEd1a4ISteLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}